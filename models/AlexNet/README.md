# Tensorflow 实现AlexNet

## AlexNet中的技术
* 使用ReLU作为激活函数，并说明其效果在深层网络中优于Sigmoid函数，避免了网络太深导致的梯度弥散问题
* 使用Dropout来避免模型过拟合
* 使用最大池化而不用平均池化，避免了平均池化导致的模糊效果。并且让步长比卷积核的长度小，提升了特征的丰富性。
* 提出了LRN层，对局部神经元的活动创建了竞争机制，让响应较大的值变得相对更大，并抑制反馈较小的神经元，增强了模型的泛化能力.
* 使用CUDA加速网络训练。
* 数据增强，随机从256\*256的原始图像中截取224\*224的大小，水平翻转。预测时取图片的四个角和中间共５个位置，并进行左右翻转得到10张图片，对其进行预测并对10次的结果求平均值。另外论文中提到对图像的RGB数据进行了PCA处理，并对主成分做了一个标准差为0.1的高斯噪音。

