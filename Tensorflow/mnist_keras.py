# -*- coding: utf-8 -*-
"""mnist_keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gg_wy_rdPkbAsGinzzXQolxdBSSKMgIx
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers
import tensorflow_datasets as tfds

print(tf.VERSION)
print(tf.keras.__version__)
print(tf.test.gpu_device_name())

"""获取 mnist 数据集的结构信息"""

mnist = tfds.image.MNIST()
print(mnist.info)

"""下载并准备数据集"""

mnist.download_and_prepare()

dataset = mnist.as_dataset()

assert isinstance(dataset["train"], tf.data.Dataset)

train_data = dataset["train"].batch(16).make_one_shot_iterator().get_next()
test_data = dataset["test"].batch(16).make_one_shot_iterator().get_next()

"""查看数据"""

sess = tf.Session()
images = np.zeros((28 * 4, 28 * 4), dtype=np.float32)
labels = []
train_data_val = sess.run(train_data)
for i in range(16):
  images[(i // 4) * 28:(i // 4 + 1) * 28, (i % 4) * 28:(i % 4 + 1) * 28] = train_data_val["image"][i, ..., 0]
  labels.append(train_data_val["label"][i])
print(labels)
plt.imshow(images, cmap="gray")
plt.colorbar()
plt.grid(False)
plt.show()

"""构建训练和验证数据集"""

def parse_fn(data):
  return (tf.cast(data["image"], dtype=tf.float32) / 255., data["label"])

train_ds = dataset["train"].repeat().shuffle(buffer_size=60000).map(parse_fn).batch(32).prefetch(1000)
test_ds = dataset["test"].repeat().map(parse_fn).batch(32).prefetch(1000)

"""构建 Keras 序列式模型"""

def model_fn():
  model = tf.keras.Sequential()
  model.add(layers.Conv2D(32, 5, padding="same", activation="relu"))
  model.add(layers.MaxPool2D(2, 2, padding="same"))
  model.add(layers.Conv2D(64, 5, padding="same", activation="relu"))
  model.add(layers.MaxPool2D(2, 2, padding="same"))
  model.add(layers.Flatten())
  model.add(layers.Dense(1024, activation="relu"))
  model.add(layers.Dropout(0.5))
  model.add(layers.Dense(10, activation='softmax'))
  return model

model = model_fn()
model.compile(optimizer=tf.train.AdamOptimizer(1e-4),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(train_ds, batch_size=32, epochs=10, steps_per_epoch=60000 // 32, validation_data=test_ds, validation_steps=10000 // 32)

model.evaluate(test_ds, steps=10000 // 32)

def parse_fn2(data):
  return tf.cast(data["image"], tf.float32) / 255.

pred_ds = dataset["test"].repeat().map(parse_fn2).batch(32).prefetch(1000)
model.predict(test_ds, steps=1).argmax(axis=1)

